{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_test",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkjdinesh/Data-Science-RProgramming/blob/main/GAN_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1viq_BibypHi",
        "outputId": "fd93f636-7815-4f6a-e01c-d2a3583f5132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLGl4tPAcP-l",
        "outputId": "d6e1ecaa-90e2-494e-8cf9-57115c74bd8b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HnARwGkhAMy",
        "outputId": "0a4e1ea5-aac6-4762-c423-51fb8de449ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Rakshith-Manandi/text-to-image-using-GAN.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fKekFephCkR",
        "outputId": "6de5bf10-b9d6-4d64-acdc-f5c5aa0c66a8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'text-to-image-using-GAN'...\n",
            "remote: Enumerating objects: 9073, done.\u001b[K\n",
            "remote: Total 9073 (delta 0), reused 0 (delta 0), pack-reused 9073\u001b[K\n",
            "Receiving objects: 100% (9073/9073), 1.21 GiB | 22.54 MiB/s, done.\n",
            "Resolving deltas: 100% (181/181), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/text-to-image-using-GAN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbWzjOwci3cY",
        "outputId": "ea91170c-bcc8-40d2-b25d-1bb4e3f9d54a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/text-to-image-using-GAN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14gtttq6jLIK",
        "outputId": "03c0e4e3-9863-4b9d-b579-fe784d2c93ba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoints   GAN_Demo.ipynb  __pycache__\ttrainer_demo.py\n",
            "config.yaml   gan_factory.py  README.md\t\ttrainer.py\n",
            "demo_text.py  images\t      requirements.txt\ttxt2image_dataset.py\n",
            "gan_cls.py    models\t      runtime.py\tutils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python utils.py"
      ],
      "metadata": {
        "id": "TNj0ThFNxGrO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python gan_cls.py"
      ],
      "metadata": {
        "id": "RjL3Yso5EHjC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python trainer.py"
      ],
      "metadata": {
        "id": "PV21pvxGESAD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python config.yaml"
      ],
      "metadata": {
        "id": "UYgFO_L_EclE"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python txt2image_dataset.py\n"
      ],
      "metadata": {
        "id": "82sU1AaDEg15"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/text-to-image-using-GAN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ChJSUMkxLO3",
        "outputId": "57a20cf2-1c97-448b-8afb-38286bd55a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/text-to-image-using-GAN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIHP0qupicju",
        "outputId": "9a7de455-5036-4aef-fe1b-fb24cb9a4e38"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoints   GAN_Demo.ipynb  __pycache__\ttrainer_demo.py\n",
            "config.yaml   gan_factory.py  README.md\t\ttrainer.py\n",
            "demo_text.py  images\t      requirements.txt\ttxt2image_dataset.py\n",
            "gan_cls.py    models\t      runtime.py\tutils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python gan_cls.py  gan_factory.py"
      ],
      "metadata": {
        "id": "1AKfU3OHxOgA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/text-to-image-using-GAN/models/gan_cls.py\" \"/content/text-to-image-using-GAN\""
      ],
      "metadata": {
        "id": "shynZf4a8h_x"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/text-to-image-using-GAN/models/gan_factory.py\" \"/content/text-to-image-using-GAN\""
      ],
      "metadata": {
        "id": "tNKZHNbp9VFg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python demo_text.py"
      ],
      "metadata": {
        "id": "Yi81-D-39n6K"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python utils.py gan_cls.py gan_factory.py trainer.py runtime.py txt2image_dataset.py demo_text.py"
      ],
      "metadata": {
        "id": "zmY5Q1WM_KAi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python gan_cls.py requirements.txt  trainer.py config.yaml runtime.py txt2image_dataset.py demo_text.py gan_factory.py trainer_demo.py utils.py"
      ],
      "metadata": {
        "id": "H0qefbNb_nWS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python GAN_Demo.ipynb "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTLr6X4SCJ4S",
        "outputId": "bf639182-3236-4d2f-d480-db5450ed0ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"GAN_Demo.ipynb\", line 331, in <module>\n",
            "    \"execution_count\": null,\n",
            "NameError: name 'null' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " cd /text-to-image-using-GAN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRr3Ri13A7AT",
        "outputId": "1053dd88-9f0f-40ca-abf7-a164deab54bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/text-to-image-using-GAN'\n",
            "/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python GAN_Demo.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPfT9s4EBl5L",
        "outputId": "006af1fc-20be-47d4-cf56-3b11d1220347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"GAN_Demo.ipynb\", line 331, in <module>\n",
            "    \"execution_count\": null,\n",
            "NameError: name 'null' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/text-to-image-using-GAN/utils.py\""
      ],
      "metadata": {
        "id": "PeyBZrOEB58L"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generator)"
      ],
      "metadata": {
        "id": "WQBeb02tQubO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install EasyDict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kEpInPbFVGd",
        "outputId": "41f7a7c0-91d1-444e-bfa5-80a3de7f3a7f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: EasyDict in /usr/local/lib/python3.7/dist-packages (1.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BSXwdNANah1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv_OznwUEuQc",
        "outputId": "01683360-f045-495b-fdf6-feb90a3443e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGAJfGnGLK2v",
        "outputId": "b5fe59f1-f4d7-4eaf-c5ca-040e8da7f3a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " data_loader.py\t\t generator.py   README.md\n",
            " discriminator.py\t LICENSE        skipthoughts.py\n",
            "'FlickR Serialization'\t __pycache__    train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('results_demo/original_images'):\n",
        "    os.makedirs('results_demo/original_images')"
      ],
      "metadata": {
        "id": "k1evpwwnaoIG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install easydict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVdsEL5YhlVl",
        "outputId": "70b13c1d-8d82-40ca-995a-055af5fd7bcb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (1.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python trainer.py"
      ],
      "metadata": {
        "id": "Gt4_eeE4h1Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import easydict"
      ],
      "metadata": {
        "id": "X5ZRUPcahtuY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Birds CUB dataset\n",
        "birds_dataset_path: '/content/drive/MyDrive/datasets/birds.hdf5'\n",
        "\n",
        "\n",
        "# flowers dataset\n",
        "flowers_dataset_path: '/content/drive/MyDrive/datasets/flowers.hdf5'"
      ],
      "metadata": {
        "id": "ITJP3RLnWH-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gan_cls\n",
        "\n",
        "# coding: utf-8\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "# import sys\n",
        "# #sys.path.insert(0, \"/datasets/home/23/223/rmanandi/text-to-image-using-GAN/\")\n",
        "# import nbimporter\n",
        "\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import utils\n",
        "import pdb\n",
        "\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "class generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(generator, self).__init__()\n",
        "        self.image_size = 64\n",
        "        self.num_channels = 3\n",
        "        self.noise_dim = 100\n",
        "        self.embed_dim = 1024\n",
        "        self.projected_embed_dim = 128\n",
        "        self.latent_dim = self.noise_dim + self.projected_embed_dim\n",
        "        self.ngf = 64\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(in_features=self.embed_dim, out_features=self.projected_embed_dim),\n",
        "            nn.BatchNorm1d(num_features=self.projected_embed_dim),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "            )\n",
        "\n",
        "        self.netG = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.latent_dim, self.ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(self.ngf * 2,self.ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d(self.ngf, self.num_channels, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "             # state size. (num_channels) x 64 x 64\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, embed_vector, z):\n",
        "\n",
        "        projected_embed = self.projection(embed_vector).unsqueeze(2).unsqueeze(3)\n",
        "        latent_vector = torch.cat([projected_embed, z], 1)\n",
        "        output = self.netG(latent_vector)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "class discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(discriminator, self).__init__()\n",
        "        self.image_size = 64\n",
        "        self.num_channels = 3\n",
        "        self.embed_dim = 1024\n",
        "        self.projected_embed_dim = 128\n",
        "        self.ndf = 64\n",
        "        self.B_dim = 128\n",
        "        self.C_dim = 16\n",
        "\n",
        "        self.netD_1 = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(self.num_channels, self.ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(self.ndf, self.ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(self.ndf * 2, self.ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(self.ndf * 4, self.ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "        self.projector = utils.Concat_embed(self.embed_dim, self.projected_embed_dim)\n",
        "\n",
        "        self.netD_2 = nn.Sequential(\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(self.ndf * 8 + self.projected_embed_dim, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "    def forward(self, inp, embed):\n",
        "        x_intermediate = self.netD_1(inp)\n",
        "        x = self.projector(x_intermediate, embed)\n",
        "        x = self.netD_2(x)\n",
        "\n",
        "        return x.view(-1, 1).squeeze(1) , x_intermediate\n",
        "\n"
      ],
      "metadata": {
        "id": "83gIW98eVhOZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gan factory\n",
        "\n",
        "# coding: utf-8\n",
        "\n",
        "# In[6]:\n",
        "\n",
        "\n",
        "# import sys\n",
        "# sys.path.insert(0, \"/datasets/home/30/930/dmajumde/text-to-image-using-GAN/\")\n",
        "\n",
        "\n",
        "# In[7]:\n",
        "\n",
        "\n",
        "# import nbimporter\n",
        "\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "#from models import gan,gan_cls\n",
        "import gan_cls\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "class gan_factory(object):\n",
        "\n",
        "    @staticmethod\n",
        "    def generator_factory(type):\n",
        "        if type == 'gan':\n",
        "            return gan_cls.generator()\n",
        "\n",
        "    @staticmethod\n",
        "    def discriminator_factory(type):\n",
        "        if type == 'gan':\n",
        "            return gan_cls.discriminator()\n",
        "\n"
      ],
      "metadata": {
        "id": "jLI0RCKQVpD3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flowers_samples = ['this_flower_has_a_large_number_of_light_pink_petals_that_have_irregular_edges.',\n",
        "                  'the_flower_has_soft_petals_that_are_white_and_yellow_in_color.',\n",
        "                  'this_flower_has_large_and_wide_petals_in_a_shade_of_brilliant_red.',\n",
        "                  'this_flower_has_very_smooth,_rounded_blossoms_of_pale_yellow_petals.']\n",
        "\n",
        "birds_samples = ['small_bird_with_proportionate_head_and_a_small,pointed_bill.',\n",
        "                 'this_bird_has_a_large_head_and_a_small_body_and_has_a_grey_and_black_wingspan_and_tail.',\n",
        "                 'a_small_bird_that_is_predominantly_grey_in_color_except_for_its_white_wingbras,_and_its_white_belly_',\n",
        "                 'this_small_bird_has_a_grey_bill_and_crown_and_grey_wings_with_white_wingbars.']\n"
      ],
      "metadata": {
        "id": "pqcdhAZjVtqg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Birds CUB dataset\n",
        "birds_dataset_path: '/content/drive/MyDrive/datasets/birds.hdf5'\n",
        "\n",
        "\n",
        "# flowers dataset\n",
        "flowers_dataset_path: '/content/drive/MyDrive/datasets/flowers.hdf5'\n"
      ],
      "metadata": {
        "id": "oxzL4-FKV264"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# coding: utf-8\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, \"../text-to-image-using-GAN/models\")\n",
        "# import nbimporter\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import yaml\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from txt2image_dataset import Text2ImageDataset    #txt2image_dataset.ipynb file\n",
        "from gan_factory import gan_factory\n",
        "from utils import Utils, Logger    #utils.ipynb file\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, type, dataset, split, lr, \n",
        "                 save_path, l1_coef, l2_coef, pre_trained_gen, pre_trained_disc, batch_size, num_workers, epochs):\n",
        "        with open('config.yaml', 'r') as f:\n",
        "            config = yaml.load(f)\n",
        "\n",
        "        self.generator = torch.nn.DataParallel(gan_factory.generator_factory(type).cuda())\n",
        "        self.discriminator = torch.nn.DataParallel(gan_factory.discriminator_factory(type).cuda())\n",
        "\n",
        "        if pre_trained_disc:\n",
        "            self.discriminator.load_state_dict(torch.load(pre_trained_disc))\n",
        "        else:\n",
        "            self.discriminator.apply(Utils.weights_init)\n",
        "\n",
        "        if pre_trained_gen:\n",
        "            self.generator.load_state_dict(torch.load(pre_trained_gen))\n",
        "        else:\n",
        "            self.generator.apply(Utils.weights_init)\n",
        "\n",
        "        if dataset == 'birds':\n",
        "            self.dataset = Text2ImageDataset(config['birds_dataset_path'], split=split)\n",
        "        elif dataset == 'flowers':\n",
        "            self.dataset = Text2ImageDataset(config['flowers_dataset_path'], split=split)\n",
        "        else:\n",
        "            print('Dataset not supported, please select either birds or flowers.')\n",
        "            exit()\n",
        "        \n",
        "        #print \"Image = \",len(self.dataset)\n",
        "        self.noise_dim = 100\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.lr = lr\n",
        "        self.beta1 = 0.5\n",
        "        self.num_epochs = epochs\n",
        "\n",
        "\n",
        "        self.l1_coef = l1_coef\n",
        "        self.l2_coef = l2_coef\n",
        "\n",
        "        self.data_loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True,\n",
        "                                num_workers=self.num_workers)\n",
        "\n",
        "        self.optimD = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(self.beta1, 0.999))\n",
        "        self.optimG = torch.optim.Adam(self.generator.parameters(), lr=self.lr, betas=(self.beta1, 0.999))\n",
        "\n",
        "        self.logger = Logger()\n",
        "        self.checkpoints_path = 'checkpoints'\n",
        "        self.save_path = save_path\n",
        "        self.type = type\n",
        "\n",
        "    def train(self, cls):\n",
        "\n",
        "        if self.type == 'gan':\n",
        "            self._train_gan(cls)\n",
        "\n",
        "\n",
        "    def _train_gan(self, cls):\n",
        "        criterion = nn.BCELoss()\n",
        "        l2_loss = nn.MSELoss()\n",
        "        l1_loss = nn.L1Loss()\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            iteration = 0\n",
        "            #print 'Outside Dataloader loop is'\n",
        "            for sample in self.data_loader:\n",
        "                #print 'Inside Dataloader loop is'\n",
        "                iteration += 1\n",
        "                right_images = sample['right_images']\n",
        "                right_embed = sample['right_embed']\n",
        "                wrong_images = sample['wrong_images']\n",
        "\n",
        "                right_images = Variable(right_images.float()).cuda()\n",
        "                right_embed = Variable(right_embed.float()).cuda()\n",
        "                wrong_images = Variable(wrong_images.float()).cuda()\n",
        "\n",
        "                real_labels = torch.ones(right_images.size(0))\n",
        "                fake_labels = torch.zeros(right_images.size(0))\n",
        "\n",
        "                smoothed_real_labels = torch.FloatTensor(Utils.smooth_label(real_labels.numpy(), -0.1))\n",
        "\n",
        "                real_labels = Variable(real_labels).cuda()\n",
        "                smoothed_real_labels = Variable(smoothed_real_labels).cuda()\n",
        "                fake_labels = Variable(fake_labels).cuda()\n",
        "\n",
        "                # Train the discriminator\n",
        "                self.discriminator.zero_grad()\n",
        "                outputs, activation_real = self.discriminator(right_images, right_embed)\n",
        "                real_loss = criterion(outputs, smoothed_real_labels)\n",
        "                real_score = outputs\n",
        "\n",
        "                if cls:\n",
        "                    outputs, _ = self.discriminator(wrong_images, right_embed)\n",
        "                    wrong_loss = criterion(outputs, fake_labels)\n",
        "                    wrong_score = outputs\n",
        "\n",
        "                noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
        "                noise = noise.view(noise.size(0), 100, 1, 1)\n",
        "                fake_images = self.generator(right_embed, noise)\n",
        "                outputs, _ = self.discriminator(fake_images, right_embed)\n",
        "                fake_loss = criterion(outputs, fake_labels)\n",
        "                fake_score = outputs\n",
        "\n",
        "                d_loss = real_loss + fake_loss\n",
        "\n",
        "                if cls:\n",
        "                    d_loss = d_loss + wrong_loss\n",
        "\n",
        "                d_loss.backward()\n",
        "                self.optimD.step()\n",
        "\n",
        "                # Train the generator\n",
        "                self.generator.zero_grad()\n",
        "                noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
        "                noise = noise.view(noise.size(0), 100, 1, 1)\n",
        "                fake_images = self.generator(right_embed, noise)\n",
        "                outputs, activation_fake = self.discriminator(fake_images, right_embed)\n",
        "                _, activation_real = self.discriminator(right_images, right_embed)\n",
        "\n",
        "                activation_fake = torch.mean(activation_fake, 0)    #try with median and check if it converges\n",
        "                activation_real = torch.mean(activation_real, 0)    #try with median and check if it converges\n",
        "\n",
        "\n",
        "                g_loss = criterion(outputs, real_labels)                          + self.l2_coef * l2_loss(activation_fake, activation_real.detach())                          + self.l1_coef * l1_loss(fake_images, right_images)\n",
        "\n",
        "                g_loss.backward()\n",
        "                self.optimG.step()\n",
        "                \n",
        "#                print('iter:', iteration)\n",
        "\n",
        "                if iteration % 5 == 0:\n",
        "                    self.logger.log_iteration_gan(epoch, iteration, d_loss, g_loss, real_score, fake_score)\n",
        "\n",
        "\n",
        "            if (epoch) % 10 == 0:\n",
        "                #print('epoch', epoch, 'complete')\n",
        "                Utils.save_checkpoint(self.discriminator, self.generator, self.checkpoints_path, self.save_path, epoch)\n",
        "\n",
        "\n",
        "    def predict(self):\n",
        "        for sample in self.data_loader:\n",
        "            #print len(sample)\n",
        "            right_images = sample['right_images']\n",
        "            right_embed = sample['right_embed']\n",
        "            #print type(right_embed)\n",
        "            #print right_embed.shape\n",
        "            #break\n",
        "            txt = sample['txt']\n",
        "\n",
        "            if not os.path.exists('results/{0}'.format(self.save_path)):\n",
        "                os.makedirs('results/{0}'.format(self.save_path))\n",
        "\n",
        "            right_images = Variable(right_images.float()).cuda()\n",
        "            right_embed = Variable(right_embed.float()).cuda()\n",
        "\n",
        "            # Train the generator\n",
        "            noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
        "            noise = noise.view(noise.size(0), 100, 1, 1)\n",
        "            fake_images = self.generator(right_embed, noise)\n",
        "\n",
        "\n",
        "            for image, t in zip(fake_images, txt):\n",
        "                im = Image.fromarray(image.data.mul_(127.5).add_(127.5).byte().permute(1, 2, 0).cpu().numpy())\n",
        "                im.save('results/{0}/{1}.jpg'.format(self.save_path, t.replace(\"/\", \"\")[:100]))\n",
        "                print(t)\n",
        "            break\n",
        "\n"
      ],
      "metadata": {
        "id": "qQsfI2c7V7XK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# coding: utf-8\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, \"../text-to-image-using-GAN/models\")\n",
        "# import nbimporter\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import yaml\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from txt2image_dataset import Text2ImageDataset    #txt2image_dataset.ipynb file\n",
        "from gan_factory import gan_factory\n",
        "from utils import Utils, Logger    #utils.ipynb file\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, type, dataset, split, lr, \n",
        "                 save_path, l1_coef, l2_coef, pre_trained_gen, pre_trained_disc, batch_size, num_workers, epochs):\n",
        "        with open('config.yaml', 'r') as f:\n",
        "            config = yaml.load(f)\n",
        "\n",
        "        self.generator = torch.nn.DataParallel(gan_factory.generator_factory(type).cuda())\n",
        "        self.discriminator = torch.nn.DataParallel(gan_factory.discriminator_factory(type).cuda())\n",
        "\n",
        "        if pre_trained_disc:\n",
        "            self.discriminator.load_state_dict(torch.load(pre_trained_disc))\n",
        "        else:\n",
        "            self.discriminator.apply(Utils.weights_init)\n",
        "\n",
        "        if pre_trained_gen:\n",
        "            self.generator.load_state_dict(torch.load(pre_trained_gen))\n",
        "        else:\n",
        "            self.generator.apply(Utils.weights_init)\n",
        "\n",
        "        if dataset == 'birds':\n",
        "            self.dataset = Text2ImageDataset(config['birds_dataset_path'], split=split)\n",
        "        elif dataset == 'flowers':\n",
        "            self.dataset = Text2ImageDataset(config['flowers_dataset_path'], split=split)\n",
        "        else:\n",
        "            print('Dataset not supported, please select either birds or flowers.')\n",
        "            exit()\n",
        "        \n",
        "        #print \"Image = \",len(self.dataset)\n",
        "        self.noise_dim = 100\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.lr = lr\n",
        "        self.beta1 = 0.5\n",
        "        self.num_epochs = epochs\n",
        "\n",
        "\n",
        "        self.l1_coef = l1_coef\n",
        "        self.l2_coef = l2_coef\n",
        "\n",
        "        self.data_loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=False,\n",
        "                                num_workers=self.num_workers)\n",
        "\n",
        "        self.optimD = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(self.beta1, 0.999))\n",
        "        self.optimG = torch.optim.Adam(self.generator.parameters(), lr=self.lr, betas=(self.beta1, 0.999))\n",
        "\n",
        "        self.logger = Logger()\n",
        "        self.checkpoints_path = 'checkpoints'\n",
        "        self.save_path = save_path\n",
        "        self.type = type\n",
        "\n",
        "    def train(self, cls):\n",
        "\n",
        "        if self.type == 'gan':\n",
        "            self._train_gan(cls)\n",
        "\n",
        "\n",
        "    def _train_gan(self, cls):\n",
        "        criterion = nn.BCELoss()\n",
        "        l2_loss = nn.MSELoss()\n",
        "        l1_loss = nn.L1Loss()\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            iteration = 0\n",
        "            #print 'Outside Dataloader loop is'\n",
        "            for sample in self.data_loader:\n",
        "                #print 'Inside Dataloader loop is'\n",
        "                iteration += 1\n",
        "                right_images = sample['right_images']\n",
        "                right_embed = sample['right_embed']\n",
        "                wrong_images = sample['wrong_images']\n",
        "\n",
        "                right_images = Variable(right_images.float()).cuda()\n",
        "                right_embed = Variable(right_embed.float()).cuda()\n",
        "                wrong_images = Variable(wrong_images.float()).cuda()\n",
        "\n",
        "                real_labels = torch.ones(right_images.size(0))\n",
        "                fake_labels = torch.zeros(right_images.size(0))\n",
        "\n",
        "                smoothed_real_labels = torch.FloatTensor(Utils.smooth_label(real_labels.numpy(), -0.1))\n",
        "\n",
        "                real_labels = Variable(real_labels).cuda()\n",
        "                smoothed_real_labels = Variable(smoothed_real_labels).cuda()\n",
        "                fake_labels = Variable(fake_labels).cuda()\n",
        "\n",
        "                # Train the discriminator\n",
        "                self.discriminator.zero_grad()\n",
        "                outputs, activation_real = self.discriminator(right_images, right_embed)\n",
        "                real_loss = criterion(outputs, smoothed_real_labels)\n",
        "                real_score = outputs\n",
        "\n",
        "                if cls:\n",
        "                    outputs, _ = self.discriminator(wrong_images, right_embed)\n",
        "                    wrong_loss = criterion(outputs, fake_labels)\n",
        "                    wrong_score = outputs\n",
        "\n",
        "                noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
        "                noise = noise.view(noise.size(0), 100, 1, 1)\n",
        "                fake_images = self.generator(right_embed, noise)\n",
        "                outputs, _ = self.discriminator(fake_images, right_embed)\n",
        "                fake_loss = criterion(outputs, fake_labels)\n",
        "                fake_score = outputs\n",
        "\n",
        "                d_loss = real_loss + fake_loss\n",
        "\n",
        "                if cls:\n",
        "                    d_loss = d_loss + wrong_loss\n",
        "\n",
        "                d_loss.backward()\n",
        "                self.optimD.step()\n",
        "\n",
        "                # Train the generator\n",
        "                self.generator.zero_grad()\n",
        "                noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
        "                noise = noise.view(noise.size(0), 100, 1, 1)\n",
        "                fake_images = self.generator(right_embed, noise)\n",
        "                outputs, activation_fake = self.discriminator(fake_images, right_embed)\n",
        "                _, activation_real = self.discriminator(right_images, right_embed)\n",
        "\n",
        "                activation_fake = torch.mean(activation_fake, 0)    #try with median and check if it converges\n",
        "                activation_real = torch.mean(activation_real, 0)    #try with median and check if it converges\n",
        "\n",
        "\n",
        "                g_loss = criterion(outputs, real_labels)                          + self.l2_coef * l2_loss(activation_fake, activation_real.detach())                          + self.l1_coef * l1_loss(fake_images, right_images)\n",
        "\n",
        "                g_loss.backward()\n",
        "                self.optimG.step()\n",
        "                \n",
        "#                print('iter:', iteration)\n",
        "\n",
        "                if iteration % 5 == 0:\n",
        "                    self.logger.log_iteration_gan(epoch, iteration, d_loss, g_loss, real_score, fake_score)\n",
        "\n",
        "\n",
        "            if (epoch) % 10 == 0:\n",
        "                #print('epoch', epoch, 'complete')\n",
        "                Utils.save_checkpoint(self.discriminator, self.generator, self.checkpoints_path, self.save_path, epoch)\n",
        "\n",
        "\n",
        "    def predict(self):\n",
        "        for sample in self.data_loader:\n",
        "            right_images = sample['right_images']\n",
        "            right_embed = sample['right_embed']\n",
        "            txt = sample['txt']\n",
        "\n",
        "            if not os.path.exists('results_demo/{0}'.format(self.save_path)):\n",
        "                os.makedirs('results_demo/{0}'.format(self.save_path))\n",
        "\n",
        "            right_images = Variable(right_images.float()).cuda()\n",
        "            right_embed = Variable(right_embed.float()).cuda()\n",
        "\n",
        "            # Train the generator\n",
        "            noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
        "            noise = noise.view(noise.size(0), 100, 1, 1)\n",
        "            fake_images = self.generator(right_embed, noise)\n",
        "\n",
        "            #self.logger.draw(right_images, fake_images)\n",
        "\n",
        "            for image, t in zip(fake_images, txt):\n",
        "                im = Image.fromarray(image.data.mul_(127.5).add_(127.5).byte().permute(1, 2, 0).cpu().numpy())\n",
        "                im.save('results_demo/{0}/{1}.jpg'.format(self.save_path, t.replace(\"/\", \"\").replace(\"\\n\", \"\").replace(\" \", \"_\")[:100]))\n",
        "#                print(t)\n",
        "            break\n",
        "\n"
      ],
      "metadata": {
        "id": "BdQCFQgrWA1u"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# coding: utf-8\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "import os\n",
        "import io\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pdb\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import pdb\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "class Text2ImageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, datasetFile, transform=None, split=0):\n",
        "        self.datasetFile = datasetFile\n",
        "        self.transform = transform\n",
        "        self.dataset = None\n",
        "        self.dataset_keys = None\n",
        "        self.split = 'train' if split == 0 else 'valid' if split == 1 else 'test'\n",
        "        self.h5py2int = lambda x: int(np.array(x))\n",
        "        #print 'Inside Init\\n'\n",
        "\n",
        "    def __len__(self):\n",
        "        f = h5py.File(self.datasetFile, 'r')\n",
        "        self.dataset_keys = [str(k) for k in f[self.split].keys()]\n",
        "        length = len(f[self.split])\n",
        "        f.close()\n",
        "        #print 'Inside Len\\n'\n",
        "\n",
        "        return length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.dataset is None:\n",
        "            self.dataset = h5py.File(self.datasetFile, mode='r')\n",
        "            self.dataset_keys = [str(k) for k in self.dataset[self.split].keys()]\n",
        "\n",
        "        example_name = self.dataset_keys[idx]\n",
        "        example = self.dataset[self.split][example_name]\n",
        "        #print len(example)\n",
        "\n",
        "        #print 'Inside getitem\\n'\n",
        "        #print 'Before sample\\n',idx\n",
        "        # pdb.set_trace()\n",
        "        #print \"image = \", len(example)\n",
        "        right_image = np.array(example['img']).tobytes()\n",
        "        right_embed = np.array(example['embeddings'], dtype=float)\n",
        "        #print 'Before sample',idx\n",
        "        wrong_image = np.array(self.find_wrong_image(example['class'])).tobytes()\n",
        "        inter_embed = np.array(self.find_inter_embed())\n",
        "\n",
        "        right_image = Image.open(io.BytesIO(right_image)).resize((64, 64))\n",
        "        wrong_image = Image.open(io.BytesIO(wrong_image)).resize((64, 64))\n",
        "        \n",
        "        a = example['txt'].value\n",
        "        special = u\"\\ufffd\\ufffd\"\n",
        "        a = a.replace(special,' ')\n",
        "        txt = np.array(a).astype(str)\n",
        "        txt = str(txt)\n",
        "        \n",
        "        if self.split == 'test':\n",
        "            name = txt.replace(\"/\", \"\").replace(\"\\n\", \"\").replace(\" \", \"_\")[:100]\n",
        "            right_image.save('results_demo/original_images/{0}.jpg'.format(name))\n",
        "\n",
        "        right_image = self.validate_image(right_image)\n",
        "        wrong_image = self.validate_image(wrong_image)\n",
        "        #print 'After sample',idx\n",
        "        #print 'Before sample\\n\\n',np.array(example['txt'])\n",
        "        #txt = np.array(example['txt']).astype(str)\n",
        "        #print 'Before sample\\n\\n',txt\n",
        "        sample = {\n",
        "                'right_images': torch.FloatTensor(right_image),\n",
        "                'right_embed': torch.FloatTensor(right_embed),\n",
        "                'wrong_images': torch.FloatTensor(wrong_image),\n",
        "                'inter_embed': torch.FloatTensor(inter_embed),\n",
        "                'txt': txt\n",
        "                 }\n",
        "\n",
        "        sample['right_images'] = sample['right_images'].sub_(127.5).div_(127.5)\n",
        "        sample['wrong_images'] =sample['wrong_images'].sub_(127.5).div_(127.5)\n",
        "        #print 'After sample',idx\n",
        "        return sample\n",
        "\n",
        "    def find_wrong_image(self, category):\n",
        "        idx = np.random.randint(len(self.dataset_keys))\n",
        "        example_name = self.dataset_keys[idx]\n",
        "        example = self.dataset[self.split][example_name]\n",
        "        _category = example['class']\n",
        "\n",
        "        if _category != category:\n",
        "            return example['img']\n",
        "\n",
        "        return self.find_wrong_image(category)\n",
        "\n",
        "    def find_inter_embed(self):\n",
        "        idx = np.random.randint(len(self.dataset_keys))\n",
        "        example_name = self.dataset_keys[idx]\n",
        "        example = self.dataset[self.split][example_name]\n",
        "        return example['embeddings']\n",
        "\n",
        "\n",
        "    def validate_image(self, img):\n",
        "        img = np.array(img, dtype=float)\n",
        "        if len(img.shape) < 3:\n",
        "            rgb = np.empty((64, 64, 3), dtype=np.float32)\n",
        "            rgb[:, :, 0] = img\n",
        "            rgb[:, :, 1] = img\n",
        "            rgb[:, :, 2] = img\n",
        "            img = rgb\n",
        "\n",
        "        return img.transpose(2, 0, 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "6cHoQQbZWD-j"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# coding: utf-8\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "# import nbimporter\n",
        "\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch import  autograd\n",
        "import torch\n",
        "import os\n",
        "import pdb\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "class Concat_embed(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim, projected_embed_dim):\n",
        "        super(Concat_embed, self).__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(in_features=embed_dim, out_features=projected_embed_dim),\n",
        "            nn.BatchNorm1d(num_features=projected_embed_dim),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "            )\n",
        "\n",
        "    def forward(self, inp, embed):\n",
        "        projected_embed = self.projection(embed)\n",
        "        replicated_embed = projected_embed.repeat(4, 4, 1, 1).permute(2,  3, 0, 1)\n",
        "        hidden_concat = torch.cat([inp, replicated_embed], 1)\n",
        "\n",
        "        return hidden_concat\n",
        "\n",
        "class Utils(object):\n",
        "\n",
        "    @staticmethod\n",
        "    def smooth_label(tensor, offset):\n",
        "        return tensor + offset\n",
        "\n",
        "    @staticmethod\n",
        "    def save_checkpoint(netD, netG, dir_path, subdir_path, epoch):\n",
        "        path =  os.path.join(dir_path, subdir_path)\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "\n",
        "        torch.save(netD.state_dict(), '{0}/disc_{1}.pth'.format(path, epoch))\n",
        "        torch.save(netG.state_dict(), '{0}/gen_{1}.pth'.format(path, epoch))\n",
        "\n",
        "    @staticmethod\n",
        "    def weights_init(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            m.weight.data.normal_(0.0, 0.02)\n",
        "        elif classname.find('BatchNorm') != -1:\n",
        "            m.weight.data.normal_(1.0, 0.02)\n",
        "            m.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "\n",
        "    def log_iteration_gan(self, epoch, iteration, d_loss, g_loss, real_score, fake_score):\n",
        "        print(\"Epoch: %d, Iter: %d, d_loss= %f, g_loss= %f, D(X)= %f, D(G(X))= %f\" % (\n",
        "            epoch, iteration, d_loss.data.cpu().mean(), g_loss.data.cpu().mean(), real_score.data.cpu().mean(),\n",
        "            fake_score.data.cpu().mean()))"
      ],
      "metadata": {
        "id": "gTxl1GAGWKnT"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# coding: utf-8\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, \"../text-to-image-using-GAN/models\")\n",
        "# import nbimporter\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import yaml\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from txt2image_dataset import Text2ImageDataset    #txt2image_dataset.ipynb file\n",
        "from gan_factory import gan_factory\n",
        "from utils import Utils, Logger    #utils.ipynb file\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, type, dataset, split, lr, \n",
        "                 save_path, l1_coef, l2_coef, pre_trained_gen, pre_trained_disc, batch_size, num_workers, epochs):\n",
        "        with open('config.yaml', 'r') as f:\n",
        "            config = yaml.load(f)\n",
        "\n",
        "        self.generator = torch.nn.DataParallel(gan_factory.generator_factory(type).cuda())\n",
        "        self.discriminator = torch.nn.DataParallel(gan_factory.discriminator_factory(type).cuda())\n",
        "\n",
        "        if pre_trained_disc:\n",
        "            self.discriminator.load_state_dict(torch.load(pre_trained_disc))\n",
        "        else:\n",
        "            self.discriminator.apply(Utils.weights_init)\n",
        "\n",
        "        if pre_trained_gen:\n",
        "            self.generator.load_state_dict(torch.load(pre_trained_gen))\n",
        "        else:\n",
        "            self.generator.apply(Utils.weights_init)\n",
        "\n",
        "        if dataset == 'birds':\n",
        "            self.dataset = Text2ImageDataset(config['birds_dataset_path'], split=split)\n",
        "        elif dataset == 'flowers':\n",
        "            self.dataset = Text2ImageDataset(config['flowers_dataset_path'], split=split)\n",
        "        else:\n",
        "            print('Dataset not supported, please select either birds or flowers.')\n",
        "            exit()\n",
        "        \n",
        "        #print \"Image = \",len(self.dataset)\n",
        "        self.noise_dim = 100\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.lr = lr\n",
        "        self.beta1 = 0.5\n",
        "        self.num_epochs = epochs\n",
        "\n",
        "\n",
        "        self.l1_coef = l1_coef\n",
        "        self.l2_coef = l2_coef\n",
        "\n",
        "        self.data_loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=False,\n",
        "                                num_workers=self.num_workers)\n",
        "\n",
        "        self.optimD = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(self.beta1, 0.999))\n",
        "        self.optimG = torch.optim.Adam(self.generator.parameters(), lr=self.lr, betas=(self.beta1, 0.999))\n",
        "\n",
        "        self.logger = Logger()\n",
        "        self.checkpoints_path = 'checkpoints'\n",
        "        self.save_path = save_path\n",
        "        self.type = type\n",
        "\n",
        "    def train(self, cls):\n",
        "\n",
        "        if self.type == 'gan':\n",
        "            self._train_gan(cls)\n",
        "\n",
        "\n",
        "    def _train_gan(self, cls):\n",
        "        criterion = nn.BCELoss()\n",
        "        l2_loss = nn.MSELoss()\n",
        "        l1_loss = nn.L1Loss()\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            iteration = 0\n",
        "            #print 'Outside Dataloader loop is'\n",
        "            for sample in self.data_loader:\n",
        "                #print 'Inside Dataloader loop is'\n",
        "                iteration += 1\n",
        "                right_images = sample['right_images']\n",
        "                right_embed = sample['right_embed']\n",
        "                wrong_images = sample['wrong_images']\n",
        "\n",
        "                right_images = Variable(right_images.float()).cuda()\n",
        "                right_embed = Variable(right_embed.float()).cuda()\n",
        "                wrong_images = Variable(wrong_images.float()).cuda()\n",
        "\n",
        "                real_labels = torch.ones(right_images.size(0))\n",
        "                fake_labels = torch.zeros(right_images.size(0))\n",
        "\n",
        "                smoothed_real_labels = torch.FloatTensor(Utils.smooth_label(real_labels.numpy(), -0.1))\n",
        "\n",
        "                real_labels = Variable(real_labels).cuda()\n",
        "                smoothed_real_labels = Variable(smoothed_real_labels).cuda()\n",
        "                fake_labels = Variable(fake_labels).cuda()\n",
        "\n",
        "                # Train the discriminator\n",
        "                self.discriminator.zero_grad()\n",
        "                outputs, activation_real = self.discriminator(right_images, right_embed)\n",
        "                real_loss = criterion(outputs, smoothed_real_labels)\n",
        "                real_score = outputs\n",
        "\n",
        "                if cls:\n",
        "                    outputs, _ = self.discriminator(wrong_images, right_embed)\n",
        "                    wrong_loss = criterion(outputs, fake_labels)\n",
        "                    wrong_score = outputs\n",
        "\n",
        "                noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
        "                noise = noise.view(noise.size(0), 100, 1, 1)\n",
        "                fake_images = self.generator(right_embed, noise)\n",
        "                outputs, _ = self.discriminator(fake_images, right_embed)\n",
        "                fake_loss = criterion(outputs, fake_labels)\n",
        "                fake_score = outputs\n",
        "\n",
        "                d_loss = real_loss + fake_loss\n",
        "\n",
        "                if cls:\n",
        "                    d_loss = d_loss + wrong_loss\n",
        "\n",
        "                d_loss.backward()\n",
        "                self.optimD.step()\n",
        "\n",
        "                # Train the generator\n",
        "                self.generator.zero_grad()\n",
        "                noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
        "                noise = noise.view(noise.size(0), 100, 1, 1)\n",
        "                fake_images = self.generator(right_embed, noise)\n",
        "                outputs, activation_fake = self.discriminator(fake_images, right_embed)\n",
        "                _, activation_real = self.discriminator(right_images, right_embed)\n",
        "\n",
        "                activation_fake = torch.mean(activation_fake, 0)    #try with median and check if it converges\n",
        "                activation_real = torch.mean(activation_real, 0)    #try with median and check if it converges\n",
        "\n",
        "\n",
        "                g_loss = criterion(outputs, real_labels)                          + self.l2_coef * l2_loss(activation_fake, activation_real.detach())                          + self.l1_coef * l1_loss(fake_images, right_images)\n",
        "\n",
        "                g_loss.backward()\n",
        "                self.optimG.step()\n",
        "                \n",
        "#                print('iter:', iteration)\n",
        "\n",
        "                if iteration % 5 == 0:\n",
        "                    self.logger.log_iteration_gan(epoch, iteration, d_loss, g_loss, real_score, fake_score)\n",
        "\n",
        "\n",
        "            if (epoch) % 10 == 0:\n",
        "                #print('epoch', epoch, 'complete')\n",
        "                Utils.save_checkpoint(self.discriminator, self.generator, self.checkpoints_path, self.save_path, epoch)\n",
        "\n",
        "\n",
        "    def predict(self):\n",
        "        for sample in self.data_loader:\n",
        "            right_images = sample['right_images']\n",
        "            right_embed = sample['right_embed']\n",
        "            txt = sample['txt']\n",
        "\n",
        "            if not os.path.exists('results_demo/{0}'.format(self.save_path)):\n",
        "                os.makedirs('results_demo/{0}'.format(self.save_path))\n",
        "\n",
        "            right_images = Variable(right_images.float()).cuda()\n",
        "            right_embed = Variable(right_embed.float()).cuda()\n",
        "\n",
        "            # Train the generator\n",
        "            noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
        "            noise = noise.view(noise.size(0), 100, 1, 1)\n",
        "            fake_images = self.generator(right_embed, noise)\n",
        "\n",
        "            #self.logger.draw(right_images, fake_images)\n",
        "\n",
        "            for image, t in zip(fake_images, txt):\n",
        "                im = Image.fromarray(image.data.mul_(127.5).add_(127.5).byte().permute(1, 2, 0).cpu().numpy())\n",
        "                im.save('results_demo/{0}/{1}.jpg'.format(self.save_path, t.replace(\"/\", \"\").replace(\"\\n\", \"\").replace(\" \", \"_\")[:100]))\n",
        "#                print(t)\n",
        "            break\n",
        "\n"
      ],
      "metadata": {
        "id": "6uVZP2WSaWmF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall h5py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1B55xlcWN4y",
        "outputId": "38fff4fc-5958-4027-ae62-13689268f3b9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: h5py 2.9.0\n",
            "Uninstalling h5py-2.9.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/h5py-2.9.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/h5py/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled h5py-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py==2.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "oEXM0anBXvdz",
        "outputId": "be8cbf19-1ef8-417c-b490-541472a710a8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting h5py==2.9\n",
            "  Using cached h5py-2.9.0-cp37-cp37m-manylinux1_x86_64.whl (2.8 MB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.9) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.9) (1.21.6)\n",
            "Installing collected packages: h5py\n",
            "Successfully installed h5py-2.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py"
      ],
      "metadata": {
        "id": "pUOArPckZh01"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python -u runtime.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_w_i_6qWQVI",
        "outputId": "1020b9e8-b882-4903-8ab6-ca31961561c4"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
            "this flower has pink petals as well as a white stamen.\n",
            "\n",
            "this flower has petals that are pink and has yellow stamen\n",
            "\n",
            "the pedals of this flower are white with a long stigma\n",
            "\n",
            "this flower is purple and white in color, with petals that are multi colored.\n",
            "\n",
            "this flower has petals that are pink and has purple lines\n",
            "\n",
            "the stamen and pistils are not seen in the flower.\n",
            "\n",
            "this flower has petals that are yellow and very thin\n",
            "\n",
            "this flower has petals that are pink and has white stamen\n",
            "\n",
            "this flower has a lot of small and thin petals with a lot of anthers in the middle\n",
            "\n",
            "this flower has purple petals and a thorny green pedicel\n",
            "\n",
            "these flowers have shaggy shaped yellow leaves with yellow stamen in the center.\n",
            "\n",
            "the flower shown has yellow petals along with yellow anther and filament\n",
            "\n",
            "this flower is orange and black in color, with petals that are pointed at the tip.\n",
            "\n",
            "this flower is yellow and orange in color, and has petals that are pointed on the ends.\n",
            "\n",
            "this flower is pink and white in color, with oval shaped petals.\n",
            "\n",
            "this flower has large rounded petals that are composed of bright colors such as orange and pink.\n",
            "\n",
            "this flower has large and curved pink petals with bright yellow mouths.\n",
            "\n",
            "these purple flowers are unique with thin striped leaves and colorfulness.\n",
            "\n",
            "the petals are broad and has green sepals on its stalk.\n",
            "\n",
            "this is a pink flower that turns white toward the receptacle and has yellow stamens.\n",
            "\n",
            "a large group of white and blue flowers and a green stem.\n",
            "\n",
            "the flower has pink petals with a pink and yellow stigma and green pedicel.\n",
            "\n",
            "the petals of the flower are ruffled in appearance and are white in color.\n",
            "\n",
            "this flower has long pink petals in a ring configuration.\n",
            "\n",
            "this flower has yellow petals and a pedicel that is brown\n",
            "\n",
            "this flower is yellow in color, with petals that are very skinny.\n",
            "\n",
            "this flower has petals that are white and has a yellow center\n",
            "\n",
            "the flower has petals that are thin and yellow with a large yellow center.\n",
            "\n",
            "this drooping flower is yellow with three very separate petals and larger light yellow stigma and stamens. there is also a dark green bud that has not yet flowered.\n",
            "\n",
            "this flower has numerous purple stamen surrounded by long white petals with purple centers.\n",
            "\n",
            "this flower has pointy long yellow petals and one sepal\n",
            "\n",
            "the flower has bell shaped purple petals with white stamen\n",
            "\n",
            "this flower is pink and red in color, with petals that are layered.\n",
            "\n",
            "this flower is white and yellow in color, with only one large petal.\n",
            "\n",
            "flower with light and dark purple petals growing upward\n",
            "\n",
            "this flower has a very beautiful peach colored tint on it's petals\n",
            "\n",
            "this flower has rows of elongated shiny petals with a large round, brown center.\n",
            "\n",
            "this flower has purple, bell-shaped petals and a long, green pistil.\n",
            "\n",
            "this flower has petals that are yellow and has dark lines\n",
            "\n",
            "this flower has long pink petals in a ring configuration.\n",
            "\n",
            "the tips of the petals are pink in color and have green leaves.\n",
            "\n",
            "this flower has petals that are orange and has black dots\n",
            "\n",
            "this flower is purple in color, with petals that are bulb shaped.\n",
            "\n",
            "this flower is pink in color, and has petals that are wavy and thin.\n",
            "\n",
            "the petals of the flower are red in color and have leaves that are green.\n",
            "\n",
            "this flower is yellow in color, with petals that are curled inward.\n",
            "\n",
            "the flower has purple fragile looking petals with dotted lines decreasing in size towards the receptacle\n",
            "\n",
            "this flower is pink and red in color, with petals that are oval shaped.\n",
            "\n",
            "this flower has a white funnel shaped petal with a yellow pistil.\n",
            "\n",
            "this flower is pink and red in color, with petals that are drooping down.\n",
            "\n",
            "this flower has petals that are pink with yellow lines\n",
            "\n",
            "this flower has one long purple petal and three shorter purple petals with yellow and black striping; they surround a yellow tipped pistil\n",
            "\n",
            "the petals are spiky in appearance and are magenta in color.\n",
            "\n",
            "the flower has four pale pink petals with dark pink veins and neon yellow pistils and in the center.\n",
            "\n",
            "an outer row of large, round white petals with an inner row of small, pointed, white petals that have a yellow stripe down the center.\n",
            "\n",
            "this flower has multicolored petals that are varied shades of peach and yellow.\n",
            "\n",
            "this flower is yellow in color, with petals that are rounded.\n",
            "\n",
            "this flower has very long stamen with large red anther and has large orange petals that have red dots on them.\n",
            "\n",
            "this flower has petals that are pink with pointy ends and yellow and white stamen\n",
            "\n",
            "the beautiful yellow flower is like a ball of sunshine with it's bright petals and dark green leaves.\n",
            "\n",
            "this flower has petals that are yellow with black lines\n",
            "\n",
            "this flower is yellow and white in color, with petals that are striped near the center.\n",
            "\n",
            "the flower has skinny dark pink petals and a large round pistil.\n",
            "\n",
            "this is a bright yellow flower with almost a golden tinge to it, the middle is the same color. there are many long, fan shaped petals which for a circular shape stemming from the middle. each petal has notches at the ends of them.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy5ySWU2tKnF",
        "outputId": "7d2b9622-57fe-4a5a-c8d0-2a9da76327ff"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "fsrWvZFzTZlB"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy"
      ],
      "metadata": {
        "id": "0xSiKgVXTfPp"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import easydict"
      ],
      "metadata": {
        "id": "HYZU1zYdThXp"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image"
      ],
      "metadata": {
        "id": "FlaB9iKfTkIi"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch==0.3.1.post3\n",
        "h5py==2.7.1\n",
        "easydict==1.9\n",
        "numpy==1.14.2\n",
        "Pillow==5.3.0\n",
        "PyYAML==3.13"
      ],
      "metadata": {
        "id": "sl4e-VRUS-1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python torch == 0.3.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgLWpwYhTBjB",
        "outputId": "9fffbdff-a8cd-4b37-941e-0b00476e009c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file 'torch': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!python GAN_Demo.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JvCwTOOaDXR",
        "outputId": "37197dbb-9d50-44c4-9a4f-6347e9925255"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"GAN_Demo.ipynb\", line 331, in <module>\n",
            "    \"execution_count\": null,\n",
            "NameError: name 'null' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# coding: utf-8\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "# import nbimporter\n",
        "# import sys\n",
        "\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "# sys.tracebacklimit = 0\n",
        "\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "from trainer import Trainer     #trainer.ipynb file\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import argparse\n",
        "from PIL import Image   #This may not be used\n",
        "import os    ##This may not be used\n",
        "import easydict\n",
        "\n",
        "args = easydict.EasyDict({'type': 'gan', \n",
        "                         'lr': 0.0002,\n",
        "                         'l1_coef': 50,\n",
        "                         'l2_coef': 100,\n",
        "                         'cls': True,\n",
        "                         'save_path':'./content/text-to-image-using-GAN/checkpoints/flowers_cls',\n",
        "\n",
        "'inference': True,\n",
        "'pre_trained_disc': 'checkpoints/flowers_cls/disc_190.pth',\n",
        "'pre_trained_gen': 'checkpoints/flowers_cls/gen_190.pth',\n",
        "'dataset': 'flowers', \n",
        "'split': 2,\n",
        "'batch_size':64,\n",
        "'num_workers':8,\n",
        "'epochs':200})\n",
        "\n",
        "trainer = Trainer(type=args.type,\n",
        "                  dataset=args.dataset,\n",
        "                  split=args.split,\n",
        "                  lr=args.lr,\n",
        "                  save_path=args.save_path,\n",
        "                  l1_coef=args.l1_coef,\n",
        "                  l2_coef=args.l2_coef,\n",
        "                  pre_trained_disc=args.pre_trained_disc,\n",
        "                  pre_trained_gen=args.pre_trained_gen,\n",
        "                  batch_size=args.batch_size,\n",
        "                  num_workers=args.num_workers,\n",
        "                  epochs=args.epochs\n",
        "                  )\n",
        "\n",
        "if not args.inference:\n",
        "    trainer.train(args.cls)\n",
        "else:\n",
        "    trainer.predict()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "QRejai8zam5X",
        "outputId": "640759bc-d8d0-4784-9a7c-e4d964b8a0e2"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-76a3ccefe942>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/text-to-image-using-GAN/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;31m#print len(sample)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mright_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'right_images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/content/text-to-image-using-GAN/txt2image_dataset.py\", line 66, in __getitem__\n    a = example['txt'].value\nAttributeError: 'Dataset' object has no attribute 'value'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python GAN_Demo.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SjNH7bkWuSX",
        "outputId": "7f376610-267e-498c-c332-f24b29a40a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"GAN_Demo.ipynb\", line 331, in <module>\n",
            "    \"execution_count\": null,\n",
            "NameError: name 'null' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GAN-Demo\n",
        "from trainer_demo import Trainer     #trainer.ipynb file\n",
        "import argparse\n",
        "from PIL import Image\n",
        "import os   \n",
        "import easydict"
      ],
      "metadata": {
        "id": "hwaDved9U7Lp"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    0 : 'flowers',\n",
        "    1 : 'flowers_cls',\n",
        "    2 : 'birds',\n",
        "    3 : 'birds_cls',\n",
        "}"
      ],
      "metadata": {
        "id": "UY_XdjILVAz5"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('results_demo/original_images'):\n",
        "    os.makedirs('results_demo/original_images')"
      ],
      "metadata": {
        "id": "PzoEPP3hVEdo"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall h5py"
      ],
      "metadata": {
        "id": "M4Lf-8n2-cMk",
        "outputId": "ef2a212a-dbcd-485d-fac8-025b3e8779eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: h5py 2.9.0\n",
            "Uninstalling h5py-2.9.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/h5py-2.9.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/h5py/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled h5py-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py==2.9"
      ],
      "metadata": {
        "id": "Fum_Pq6s-Ga0",
        "outputId": "f7b43c36-2cbe-4698-9860-8fede57b818a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting h5py==2.9\n",
            "  Using cached h5py-2.9.0-cp37-cp37m-manylinux1_x86_64.whl (2.8 MB)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.9) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.9) (1.15.0)\n",
            "Installing collected packages: h5py\n",
            "Successfully installed h5py-2.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py"
      ],
      "metadata": {
        "id": "t8GURZwS-kiO"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4):\n",
        "    args = easydict.EasyDict({\n",
        "                                'type': 'gan', \n",
        "                                'lr': 0.0002,\n",
        "                                'l1_coef': 50,\n",
        "                                'l2_coef': 100,\n",
        "                                'cls': bool(i%2),\n",
        "                                'save_path': models[i],\n",
        "#                                'inference': True,\n",
        "                                'pre_trained_disc': 'checkpoints/{0}/disc_190.pth'.format(models[i]),\n",
        "                                'pre_trained_gen': 'checkpoints/{0}/gen_190.pth'.format(models[i]),\n",
        "                                'dataset': models[(i/2)*2],\n",
        "                                'split': 2,\n",
        "                                'batch_size': 64,\n",
        "                                'num_workers': 8,\n",
        "                                'epochs':200})\n",
        "\n",
        "    trainer = Trainer(type=args.type,\n",
        "                      dataset=args.dataset,\n",
        "                      split=args.split,\n",
        "                      lr=args.lr,\n",
        "                      save_path=args.save_path,\n",
        "                      l1_coef=args.l1_coef,\n",
        "                      l2_coef=args.l2_coef,\n",
        "                      pre_trained_disc=args.pre_trained_disc,\n",
        "                      pre_trained_gen=args.pre_trained_gen,\n",
        "                      batch_size=args.batch_size,\n",
        "                      num_workers=args.num_workers,\n",
        "                      epochs=args.epochs\n",
        "                      )\n",
        "\n",
        "#     if not args.inference:\n",
        "#         trainer.train(args.cls)\n",
        "#     else:\n",
        "    trainer.predict()"
      ],
      "metadata": {
        "id": "vx-MsxC47Z0b",
        "outputId": "f3c07fee-2b1b-4876-a214-41b45fc4deff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-02fc0b3dbedd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#         trainer.train(args.cls)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#     else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-52-57786f6980a9>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mright_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'right_images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'right_embed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/content/text-to-image-using-GAN/txt2image_dataset.py\", line 66, in __getitem__\n    a = example['txt'].value\nAttributeError: 'Dataset' object has no attribute 'value'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python trainer_demo.py"
      ],
      "metadata": {
        "id": "xUlNzqdhXMWp"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, \"../text-to-image-using-GAN/models\")\n",
        "# import nbimporter\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import yaml\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from txt2image_dataset import Text2ImageDataset    #txt2image_dataset.ipynb file\n",
        "from gan_factory import gan_factory\n",
        "from utils import Utils, Logger    #utils.ipynb file\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, type, dataset, split, lr, \n",
        "                 save_path, l1_coef, l2_coef, pre_trained_gen, pre_trained_disc, batch_size, num_workers, epochs):\n",
        "        with open('config.yaml', 'r') as f:\n",
        "            config = yaml.load(f)\n",
        "\n",
        "        self.generator = torch.nn.DataParallel(gan_factory.generator_factory(type).cuda())\n",
        "        self.discriminator = torch.nn.DataParallel(gan_factory.discriminator_factory(type).cuda())\n",
        "\n",
        "        if pre_trained_disc:\n",
        "            self.discriminator.load_state_dict(torch.load(pre_trained_disc))\n",
        "        else:\n",
        "            self.discriminator.apply(Utils.weights_init)\n",
        "\n",
        "        if pre_trained_gen:\n",
        "            self.generator.load_state_dict(torch.load(pre_trained_gen))\n",
        "        else:\n",
        "            self.generator.apply(Utils.weights_init)\n",
        "\n",
        "        if dataset == 'birds':\n",
        "            self.dataset = Text2ImageDataset(config['birds_dataset_path'], split=split)\n",
        "        elif dataset == 'flowers':\n",
        "            self.dataset = Text2ImageDataset(config['flowers_dataset_path'], split=split)\n",
        "        else:\n",
        "            print('Dataset not supported, please select either birds or flowers.')\n",
        "            exit()\n",
        "        \n",
        "        #print \"Image = \",len(self.dataset)\n",
        "        self.noise_dim = 100\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.lr = lr\n",
        "        self.beta1 = 0.5\n",
        "        self.num_epochs = epochs\n",
        "\n",
        "\n",
        "        self.l1_coef = l1_coef\n",
        "        self.l2_coef = l2_coef\n",
        "\n",
        "        self.data_loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=False,\n",
        "                                num_workers=self.num_workers)\n",
        "\n",
        "        self.optimD = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(self.beta1, 0.999))\n",
        "        self.optimG = torch.optim.Adam(self.generator.parameters(), lr=self.lr, betas=(self.beta1, 0.999))\n",
        "\n",
        "        self.logger = Logger()\n",
        "        self.checkpoints_path = 'checkpoints'\n",
        "        self.save_path = save_path\n",
        "        self.type = type\n",
        "\n",
        "    def train(self, cls):\n",
        "\n",
        "        if self.type == 'gan':\n",
        "            self._train_gan(cls)\n",
        "\n",
        "\n",
        "    def _train_gan(self, cls):\n",
        "        criterion = nn.BCELoss()\n",
        "        l2_loss = nn.MSELoss()\n",
        "        l1_loss = nn.L1Loss()\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            iteration = 0\n",
        "            #print 'Outside Dataloader loop is'\n",
        "            for sample in self.data_loader:\n",
        "                #print 'Inside Dataloader loop is'\n",
        "                iteration += 1\n",
        "                right_images = sample['right_images']\n",
        "                right_embed = sample['right_embed']\n",
        "                wrong_images = sample['wrong_images']\n",
        "\n",
        "                right_images = Variable(right_images.float()).cuda()\n",
        "                right_embed = Variable(right_embed.float()).cuda()\n",
        "                wrong_images = Variable(wrong_images.float()).cuda()\n",
        "\n",
        "                real_labels = torch.ones(right_images.size(0))\n",
        "                fake_labels = torch.zeros(right_images.size(0))\n",
        "\n",
        "                smoothed_real_labels = torch.FloatTensor(Utils.smooth_label(real_labels.numpy(), -0.1))\n",
        "\n",
        "                real_labels = Variable(real_labels).cuda()\n",
        "                smoothed_real_labels = Variable(smoothed_real_labels).cuda()\n",
        "                fake_labels = Variable(fake_labels).cuda()\n",
        "\n",
        "                # Train the discriminator\n",
        "                self.discriminator.zero_grad()\n",
        "                outputs, activation_real = self.discriminator(right_images, right_embed)\n",
        "                real_loss = criterion(outputs, smoothed_real_labels)\n",
        "                real_score = outputs\n",
        "\n",
        "                if cls:\n",
        "                    outputs, _ = self.discriminator(wrong_images, right_embed)\n",
        "                    wrong_loss = criterion(outputs, fake_labels)\n",
        "                    wrong_score = outputs\n",
        "\n",
        "                noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
        "                noise = noise.view(noise.size(0), 100, 1, 1)\n",
        "                fake_images = self.generator(right_embed, noise)\n",
        "                outputs, _ = self.discriminator(fake_images, right_embed)\n",
        "                fake_loss = criterion(outputs, fake_labels)\n",
        "                fake_score = outputs\n",
        "\n",
        "                d_loss = real_loss + fake_loss\n",
        "\n",
        "                if cls:\n",
        "                    d_loss = d_loss + wrong_loss\n",
        "\n",
        "                d_loss.backward()\n",
        "                self.optimD.step()\n",
        "\n",
        "                # Train the generator\n",
        "                self.generator.zero_grad()\n",
        "                noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
        "                noise = noise.view(noise.size(0), 100, 1, 1)\n",
        "                fake_images = self.generator(right_embed, noise)\n",
        "                outputs, activation_fake = self.discriminator(fake_images, right_embed)\n",
        "                _, activation_real = self.discriminator(right_images, right_embed)\n",
        "\n",
        "                activation_fake = torch.mean(activation_fake, 0)    #try with median and check if it converges\n",
        "                activation_real = torch.mean(activation_real, 0)    #try with median and check if it converges\n",
        "\n",
        "\n",
        "                g_loss = criterion(outputs, real_labels)                          + self.l2_coef * l2_loss(activation_fake, activation_real.detach())                          + self.l1_coef * l1_loss(fake_images, right_images)\n",
        "\n",
        "                g_loss.backward()\n",
        "                self.optimG.step()\n",
        "                \n",
        "#                print('iter:', iteration)\n",
        "\n",
        "                if iteration % 5 == 0:\n",
        "                    self.logger.log_iteration_gan(epoch, iteration, d_loss, g_loss, real_score, fake_score)\n",
        "\n",
        "\n",
        "            if (epoch) % 10 == 0:\n",
        "                #print('epoch', epoch, 'complete')\n",
        "                Utils.save_checkpoint(self.discriminator, self.generator, self.checkpoints_path, self.save_path, epoch)\n",
        "\n",
        "\n",
        "    def predict(self):\n",
        "        for sample in self.data_loader:\n",
        "            right_images = sample['right_images']\n",
        "            right_embed = sample['right_embed']\n",
        "            txt = sample['txt']\n",
        "\n",
        "            if not os.path.exists('results_demo/{0}'.format(self.save_path)):\n",
        "                os.makedirs('results_demo/{0}'.format(self.save_path))\n",
        "\n",
        "            right_images = Variable(right_images.float()).cuda()\n",
        "            right_embed = Variable(right_embed.float()).cuda()\n",
        "\n",
        "            # Train the generator\n",
        "            noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
        "            noise = noise.view(noise.size(0), 100, 1, 1)\n",
        "            fake_images = self.generator(right_embed, noise)\n",
        "\n",
        "            #self.logger.draw(right_images, fake_images)\n",
        "\n",
        "            for image, t in zip(fake_images, txt):\n",
        "                im = Image.fromarray(image.data.mul_(127.5).add_(127.5).byte().permute(1, 2, 0).cpu().numpy())\n",
        "                im.save('results_demo/{0}/{1}.jpg'.format(self.save_path, t.replace(\"/\", \"\").replace(\"\\n\", \"\").replace(\" \", \"_\")[:100]))\n",
        "#                print(t)\n",
        "            break"
      ],
      "metadata": {
        "id": "CXSa1_BlYhl7"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4):\n",
        "    args = easydict.EasyDict({\n",
        "                                'type': 'gan', \n",
        "                                'lr': 0.0002,\n",
        "                                'l1_coef': 50,\n",
        "                                'l2_coef': 100,\n",
        "                                'cls': bool(i%2),\n",
        "                                'save_path': models[i],\n",
        "#                                'inference': True,\n",
        "                                'pre_trained_disc': 'checkpoints/{0}/disc_190.pth'.format(models[i]),\n",
        "                                'pre_trained_gen': 'checkpoints/{0}/gen_190.pth'.format(models[i]),\n",
        "                                'dataset': models[(i/2)*2],\n",
        "                                'split': 2,\n",
        "                                'batch_size': 64,\n",
        "                                'num_workers': 8,\n",
        "                                'epochs':200})\n",
        "\n",
        "    trainer = Trainer(type=args.type,\n",
        "                      dataset=args.dataset,\n",
        "                      split=args.split,\n",
        "                      lr=args.lr,\n",
        "                      save_path=args.save_path,\n",
        "                      l1_coef=args.l1_coef,\n",
        "                      l2_coef=args.l2_coef,\n",
        "                      pre_trained_disc=args.pre_trained_disc,\n",
        "                      pre_trained_gen=args.pre_trained_gen,\n",
        "                      batch_size=args.batch_size,\n",
        "                      num_workers=args.num_workers,\n",
        "                      epochs=args.epochs\n",
        "                      )\n",
        "\n",
        "#     if not args.inference:\n",
        "#         trainer.train(args.cls)\n",
        "#     else:\n",
        "    trainer.predict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        },
        "id": "-YBJ50b0VHZY",
        "outputId": "f32c7f04-3ef6-483a-a9d0-ecb0d5a03740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
            "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset not supported, please select either birds or flowers.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-02fc0b3dbedd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                       \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                       )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-57786f6980a9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, type, dataset, split, lr, save_path, l1_coef, l2_coef, pre_trained_gen, pre_trained_disc, batch_size, num_workers, epochs)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2_coef\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         self.data_loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=False,\n\u001b[0m\u001b[1;32m     71\u001b[0m                                 num_workers=self.num_workers)\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'dataset'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"/content/text-to-image-using-GAN/GAN_Demo.ipynb\""
      ],
      "metadata": {
        "id": "RjYaKjLtBs73",
        "outputId": "04559738-fb18-468d-bd9d-905480a7a36e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: '/content/text-to-image-using-GAN/GAN_Demo.ipynb'\n",
            "Hint: It looks like a path. The path does exist. \u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python GAN_Demo.ipynb"
      ],
      "metadata": {
        "id": "ZUF-Vat1G7HC",
        "outputId": "f9ff6f68-5ee0-44b4-ab6a-3c3e024f6893",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"GAN_Demo.ipynb\", line 331, in <module>\n",
            "    \"execution_count\": null,\n",
            "NameError: name 'null' is not defined\n"
          ]
        }
      ]
    }
  ]
}